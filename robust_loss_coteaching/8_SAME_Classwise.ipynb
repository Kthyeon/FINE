{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import argparse\n",
    "import torch\n",
    "import sys\n",
    "import os\n",
    "import json\n",
    "import random\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import pickle\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt \n",
    "import sklearn.covariance\n",
    "import scipy\n",
    "import pdb\n",
    "\n",
    "import data_loader.data_loaders as module_data\n",
    "import loss as module_loss\n",
    "import model.metric as module_metric\n",
    "import model.model as module_arch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import model.model as module_arch\n",
    "\n",
    "from sklearn import metrics\n",
    "from sklearn import cluster\n",
    "from tqdm import tqdm\n",
    "from torch.autograd import Variable\n",
    "from parse_config import ConfigParser\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def return_statistics(isNoisy_list, predict):\n",
    "    r_stats = []\n",
    "    \n",
    "    tp = (isNoisy_list[predict==0]==0).sum()\n",
    "    tn = isNoisy_list[predict==1].sum()\n",
    "    fp = isNoisy_list.sum() - tn\n",
    "    fn = ((isNoisy_list.shape - isNoisy_list.sum()) - tp).item()\n",
    "    \n",
    "    precision = tp / (tp + fp)\n",
    "    recall = tp / (tp + fn)\n",
    "    specificity = tn / (tn + fp)\n",
    "    accuracy = (tp + tn) / (tp + tn + fp + fn)\n",
    "    sel_samples = int(fp + tp)\n",
    "    frac_clean = tp / (fp + tp)\n",
    "\n",
    "    r_stats.extend([sel_samples, round(precision, 5), round(recall, 5), round(specificity, 5), round(accuracy, 5), round(frac_clean, 5)])\n",
    "    \n",
    "    return r_stats\n",
    "\n",
    "# return_statistics(isNoisy_list, sing_lbl)\n",
    "\n",
    "def stat_summary(name, stat_list):\n",
    "    print('Dataset: {}, Net: {}, Noise{}_{}, Loss: {}'\n",
    "      .format(config['data_loader']['type'], config['arch']['type'], config['trainer']['asym'], config['trainer']['percent'], config['train_loss']['type']))\n",
    "    \n",
    "    print(\"=\"*50, name , \"=\"*50)\n",
    "\n",
    "    print('Selected samples by {}: {} \\nPrecision: {} \\nRecall: {} \\nSpecificity: {}\\nAccuracy: {} \\nFraction of clean samples/selected samples: {}'\n",
    "                  .format(name, stat_list[0], stat_list[1], stat_list[2], stat_list[3], stat_list[4], stat_list[5]))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Config, Model Path Setting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'name': 'cifar10_resnet34_multistep',\n",
       " 'n_gpu': 0,\n",
       " 'seed': 123,\n",
       " 'arch': {'type': 'resnet34', 'args': {'num_classes': 10}},\n",
       " 'num_classes': 10,\n",
       " 'data_loader': {'type': 'CIFAR10DataLoader',\n",
       "  'args': {'data_dir': './dir_to_data',\n",
       "   'batch_size': 128,\n",
       "   'shuffle': True,\n",
       "   'num_batches': 0,\n",
       "   'validation_split': 0,\n",
       "   'num_workers': 8,\n",
       "   'pin_memory': True}},\n",
       " 'optimizer': {'type': 'SGD',\n",
       "  'args': {'lr': 0.02, 'momentum': 0.9, 'weight_decay': 0.001}},\n",
       " 'train_loss': {'type': 'CCELoss'},\n",
       " 'val_loss': 'CrossEntropyLoss',\n",
       " 'metrics': ['my_metric', 'my_metric2'],\n",
       " 'lr_scheduler': {'type': 'MultiStepLR',\n",
       "  'args': {'milestones': [40, 80], 'gamma': 0.01}},\n",
       " 'trainer': {'epochs': 120,\n",
       "  'warmup': 0,\n",
       "  'save_dir': 'saved/',\n",
       "  'save_period': 1,\n",
       "  'verbosity': 2,\n",
       "  'label_dir': 'saved/',\n",
       "  'monitor': 'max test_my_metric',\n",
       "  'early_stop': 2000,\n",
       "  'tensorboard': False,\n",
       "  'mlflow': True,\n",
       "  '_percent': 'Percentage of noise',\n",
       "  'percent': 0.8,\n",
       "  '_begin': 'When to begin updating labels',\n",
       "  'begin': 0,\n",
       "  '_asym': 'symmetric noise if false',\n",
       "  'asym': False}}"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# config_file = './hyperparams/multistep/config_cifar10_cce_rn34.json'\n",
    "config_file = './saved/models/cifar10/resnet34/MultiStepLR/CCELoss/sym/80/config_123.json' #cifar 100\n",
    "with open(config_file, 'r') as f:\n",
    "    config = json.load(f)\n",
    "    \n",
    "device = torch.device('cuda:0')\n",
    "config['n_gpu'] = 0\n",
    "\n",
    "# resume_path = './saved/models/cifar10/resnet34/MultiStepLR/ELRLoss/sym/80/model_best123.pth'\n",
    "resume_path = './checkpoint/cifar10_rn34_multistep_sym_80_cce.pth'\n",
    "base_model = getattr(module_arch, config[\"arch\"]['type'])()\n",
    "checkpoint = torch.load(resume_path)\n",
    "state_dict = checkpoint['state_dict']\n",
    "base_model.load_state_dict(state_dict)\n",
    "\n",
    "config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n",
      "Train: 50000 Val: 0\n"
     ]
    }
   ],
   "source": [
    "# set seed\n",
    "random.seed(config['seed'])\n",
    "torch.manual_seed(config['seed'])\n",
    "torch.cuda.manual_seed_all(config['seed'])\n",
    "torch.backends.cudnn.deterministic = True\n",
    "np.random.seed(config['seed'])\n",
    "\n",
    "data_loader = getattr(module_data, config['data_loader']['type'])(\n",
    "    config['data_loader']['args']['data_dir'],\n",
    "    batch_size= 100,\n",
    "    shuffle=config['data_loader']['args']['shuffle'],\n",
    "    validation_split=0.0,\n",
    "    num_batches=config['data_loader']['args']['num_batches'],\n",
    "    training=True,\n",
    "    num_workers=config['data_loader']['args']['num_workers'],\n",
    "    pin_memory=config['data_loader']['args']['pin_memory'],\n",
    "    config=config\n",
    ")\n",
    "\n",
    "if hasattr(data_loader.dataset, 'num_raw_example'):\n",
    "    num_examp = data_loader.dataset.num_raw_example\n",
    "else:\n",
    "    num_examp = len(data_loader.dataset)\n",
    "\n",
    "critenrion = nn.CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Base ResNet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Represent(nn.Module):\n",
    "    def __init__(self, base_model):\n",
    "        super(Represent, self).__init__()\n",
    "        self.conv1 = base_model.conv1\n",
    "        self.bn1 = base_model.bn1\n",
    "        self.layer1 = base_model.layer1\n",
    "        self.layer2 = base_model.layer2\n",
    "        self.layer3 = base_model.layer3\n",
    "        self.layer4 = base_model.layer4\n",
    "        self.linear = base_model.linear\n",
    "        \n",
    "        \n",
    "    def forward(self, x):\n",
    "        out = F.relu(self.bn1(self.conv1(x)))\n",
    "        out = self.layer1(out)\n",
    "        out = self.layer2(out)\n",
    "        out = self.layer3(out)\n",
    "        out = self.layer4(out)\n",
    "        out = F.avg_pool2d(out, 4)\n",
    "        y = out.view(out.size(0), -1)\n",
    "        \n",
    "        return y\n",
    "    \n",
    "    #Feature Extractting\n",
    "    def feature_list(self, x):\n",
    "        output_list = []\n",
    "        out = F.relu(self.bn1(self.conv1(x)))\n",
    "        for name, module in self.layer1._modules.items():\n",
    "            out = module(out)\n",
    "        for name, module in self.layer2._modules.items():\n",
    "            out = module(out)\n",
    "        for name, module in self.layer3._modules.items():\n",
    "            out = module(out)\n",
    "        for name, module in self.layer4._modules.items():\n",
    "            out = module(out)\n",
    "            output_list.append(out)\n",
    "        out = F.avg_pool2d(out, 4)\n",
    "        out = out.view(out.size(0), -1)\n",
    "        y = self.linear(out)\n",
    "        return y, output_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Represent(base_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "isNoisy_list = np.empty((0,))\n",
    "isFalse_list = np.empty((0,))\n",
    "label_list = np.empty((0,))\n",
    "gt_list = np.empty((0,))\n",
    "conf_list = np.empty((0,))\n",
    "loss_list = np.empty((0,))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SAME Threshold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 500/500 [00:29<00:00, 17.01it/s]\n"
     ]
    }
   ],
   "source": [
    "model.eval()\n",
    "model.to(device)\n",
    "loss = 0\n",
    "with tqdm(data_loader) as progress:\n",
    "    for batch_idx, (data, label, index, label_gt) in enumerate(progress):\n",
    "        data = data.to(device)\n",
    "        label, label_gt = label.long().to(device), label_gt.long().to(device)\n",
    "        output = model(data)\n",
    "        _, prediction = base_model(data)\n",
    "        loss = torch.nn.CrossEntropyLoss(reduction='none')(prediction, label)\n",
    "        confidence, _ = torch.max(torch.nn.functional.softmax(prediction, dim=1), dim=1)\n",
    "        isNoisy  = label != label_gt\n",
    "        \n",
    "        gt_list = np.concatenate((gt_list, label_gt.cpu()))\n",
    "        label_list = np.concatenate((label_list, label.cpu()))\n",
    "        isNoisy_list = np.concatenate((isNoisy_list, isNoisy.cpu()))#clean = 0 noise = 1\n",
    "        conf_list = np.concatenate((conf_list, confidence.detach().cpu()))\n",
    "        loss_list = np.concatenate((loss_list, loss.detach().cpu()))\n",
    "\n",
    "        if batch_idx == 0:\n",
    "            out_list = output.detach().cpu()\n",
    "        else:\n",
    "            out_list = np.concatenate((out_list, output.detach().cpu()), axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_singular_value_vector(label_list, out_list):\n",
    "    \n",
    "    singular_dict = {}\n",
    "    v_ortho_dict = {}\n",
    "    \n",
    "    for index in np.unique(label_list):\n",
    "        u, s, v = np.linalg.svd(out_list[label_list==index])\n",
    "        singular_dict[index] = s[0] / s[1]\n",
    "        v_ortho_dict[index] = torch.from_numpy(v[:2])\n",
    "\n",
    "    return singular_dict, v_ortho_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Max eigen vector 보다 작으면 1(Noisy Label)\n",
    "def singular_label(v_ortho_dict, model_represents, label):\n",
    "    \n",
    "    model_represents = torch.from_numpy(model_represents).to(device)\n",
    "    sing_lbl = torch.zeros(model_represents.shape[0]) \n",
    "    sin_score_lbl = torch.zeros(model_represents.shape[0])\n",
    "    \n",
    "    for i, data in enumerate(model_represents):\n",
    "        sin_score_lbl[i] = torch.dot(v_ortho_dict[label[i]][0], data).abs() - torch.dot(v_ortho_dict[label[i]][1], data).abs()\n",
    "        if torch.dot(v_ortho_dict[label[i]][0], data).abs() < torch.dot(v_ortho_dict[label[i]][1], data).abs():\n",
    "            sing_lbl[i] = 1\n",
    "        \n",
    "    return sing_lbl, sin_score_lbl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def same_score(v_ortho_dict, model_represents, label):\n",
    "    \n",
    "    model_represents = torch.from_numpy(model_represents).to(device)\n",
    "    top1 = torch.zeros(model_represents.shape[0])\n",
    "    top1_abs = torch.zeros(model_represents.shape[0])\n",
    "    top12 = torch.zeros(model_represents.shape[0])\n",
    "    \n",
    "    for i, data in enumerate(model_represents):\n",
    "        top1[i] = torch.dot(v_ortho_dict[label[i]][0], data)\n",
    "        top1_abs[i] = torch.dot(v_ortho_dict[label[i]][0], data).abs()\n",
    "        top12[i] = torch.dot(v_ortho_dict[label[i]][0], data).abs() - torch.dot(v_ortho_dict[label[i]][1], data).abs()\n",
    "        \n",
    "    return top1, top1_abs, top12"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def kmean_singular_label(v_ortho_dict, model_represents, label):\n",
    "    \n",
    "    model_represents = torch.from_numpy(model_represents).to(device)\n",
    "    sing_lbl = torch.zeros(model_represents.shape[0])\n",
    "    sin_score_lbl = torch.zeros(model_represents.shape[0])\n",
    "    \n",
    "    for i, data in enumerate(model_represents):\n",
    "        sin_score_lbl[i] = torch.dot(v_ortho_dict[label[i]][0], data).abs() - torch.dot(v_ortho_dict[label[i]][1], data).abs()\n",
    "        \n",
    "    kmeans = cluster.KMeans(n_clusters=2, random_state=0).fit(sin_score_lbl.reshape(-1, 1))\n",
    "    \n",
    "    return kmeans, sin_score_lbl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "singular_dict, v_ortho_dict = get_singular_value_vector(label_list, out_list)\n",
    "\n",
    "for key in v_ortho_dict.keys():\n",
    "    v_ortho_dict[key] = v_ortho_dict[key].to(device)\n",
    "\n",
    "#For SAME Kmenas\n",
    "# ksin_score_lbl, sin_score_lbl = kmean_singular_label(v_ortho_dict, out_list, label_list)\n",
    "\n",
    "top1_score, top1_abs_score, top12_score = same_score(v_ortho_dict, out_list, label_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# (SAME) Classwise # number of clean samples(top 1%, 3% , 5%)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# class_lbl_list = []\n",
    "# class_isNoisy_list = []\n",
    "# class_sin_score = []\n",
    "# class_sel_list = [] #top-k 뽑은 것들의 noise(1) clean(0) 여부 \n",
    "\n",
    "def same_topk(lbl_list, noise_list, score_metric, topk_frac):\n",
    "    class_lbl_list, class_isNoisy_list = [], []\n",
    "    class_sin_score, class_sel_list = [], []\n",
    "\n",
    "    for i in range(0, config['num_classes']):\n",
    "        frac = int(topk_frac * np.sum(lbl_list == i))\n",
    "        \n",
    "        class_lbl_list.append(lbl_list == i)\n",
    "        tmp_isNoisy = noise_list[class_lbl_list[i]]\n",
    "        tmp_isNoisy = torch.from_numpy(tmp_isNoisy)\n",
    "        class_isNoisy_list.append(tmp_isNoisy)\n",
    "\n",
    "        class_sin_score.append(score_metric[class_lbl_list[i]])\n",
    "        tmp_sort, tmp_idx = torch.sort(class_sin_score[i], descending=True)\n",
    "\n",
    "        tmp_nc = torch.index_select(tmp_isNoisy, 0, tmp_idx[:frac])\n",
    "        \n",
    "        class_sel_list.append(tmp_nc)\n",
    "\n",
    "    return class_lbl_list, class_isNoisy_list, class_sin_score, class_sel_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 각 class 별 purity reporting 하고, 전체 purtiy 뽑아보기 / Trust set이 정말 믿을 수있는지를 판단하기 위해서."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "topk_lbl_list, topk_isNoisy_list, topk_sin_list, topk_list = same_topk(label_list, isNoisy_list, top1_abs_score, 0.01)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "#classwise clean samples fractions(clean samples / selected samples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "class 0 top 1% clean/selected: 1.0\n",
      "49.0\n",
      "49\n",
      "class 1 top 1% clean/selected: 1.0\n",
      "50.0\n",
      "50\n",
      "class 2 top 1% clean/selected: 0.9795918367346939\n",
      "48.0\n",
      "49\n",
      "class 3 top 1% clean/selected: 0.98\n",
      "49.0\n",
      "50\n",
      "class 4 top 1% clean/selected: 1.0\n",
      "50.0\n",
      "50\n",
      "class 5 top 1% clean/selected: 0.6799999999999999\n",
      "34.0\n",
      "50\n",
      "class 6 top 1% clean/selected: 0.98\n",
      "49.0\n",
      "50\n",
      "class 7 top 1% clean/selected: 1.0\n",
      "50.0\n",
      "50\n",
      "class 8 top 1% clean/selected: 1.0\n",
      "49.0\n",
      "49\n",
      "class 9 top 1% clean/selected: 1.0\n",
      "49.0\n",
      "49\n",
      "Total top 1% clean/selected: 0.9616935483870968\n",
      "496\n",
      "477.0\n"
     ]
    }
   ],
   "source": [
    "selected_data, selected_noise = 0, 0\n",
    "\n",
    "for i in range (0, len(topk_list)):\n",
    "    tmp_selected, tmp_noise = topk_list[i].shape[0], torch.sum(topk_list[i], 0)\n",
    "    \n",
    "    selected_data += tmp_selected\n",
    "    selected_noise += tmp_noise\n",
    "    print('class {} top {}% clean/selected: {}' \\\n",
    "          .format(i, round(100*tmp_selected / topk_sin_list[i].shape[0]), 1 - (tmp_noise / tmp_selected)))\n",
    "    print((tmp_selected - tmp_noise).item())\n",
    "    print(tmp_selected)\n",
    "    \n",
    "print('Total top {}% clean/selected: {}'.format(1, 1 - (selected_noise / selected_data)))\n",
    "print(selected_data)\n",
    "print((selected_data - selected_noise).item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(50000,)\n",
      "5027\n",
      "3650.0\n"
     ]
    }
   ],
   "source": [
    "# class 0\n",
    "class_0 = label_list == 5\n",
    "print(class_0.shape)\n",
    "print(np.sum(class_0))\n",
    "isNoisy_0 = isNoisy_list[class_0]\n",
    "print(np.sum(isNoisy_0))\n",
    "isNoisy_0 = torch.from_numpy(isNoisy_0)\n",
    "\n",
    "sin_score_0 = top1_abs_score[class_0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([1.9518, 1.9359, 1.8716,  ..., 0.5528, 0.5469, 0.5421])\n",
      "tensor([2676, 3015, 2944,  ..., 2650, 2022, 2700])\n"
     ]
    }
   ],
   "source": [
    "sort_0, idx_0 = torch.sort(sin_score_0, descending=True)\n",
    "print(sort_0)\n",
    "print(idx_0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([1., 1., 0., 1., 0., 0., 1., 1., 0., 0., 0., 1., 1., 0., 0., 0., 0., 1.,\n",
       "        0., 1., 0., 0., 0., 0., 1., 0., 0., 1., 0., 0., 0., 1., 1., 0., 0., 1.,\n",
       "        0., 0., 0., 1., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0.],\n",
       "       dtype=torch.float64)"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.index_select(isNoisy_0, 0, idx_0[:50])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# (Loss) Classwise # number of clean samples(top 1%, 3%, 5%)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# class_lbl_list = []\n",
    "# class_isNoisy_list = []\n",
    "# class_sin_score = []\n",
    "# class_sel_list = [] #top-k 뽑은 것들의 noise(1) clean(0) 여부 \n",
    "\n",
    "def loss_topk(lbl_list, noise_list, loss_score, topk_frac):\n",
    "    class_lbl_list, class_isNoisy_list = [], []\n",
    "    class_sin_score, class_sel_list = [], []\n",
    "\n",
    "    for i in range(0, config['num_classes']):\n",
    "        frac = int(topk_frac * np.sum(lbl_list == i))\n",
    "        \n",
    "        class_lbl_list.append(lbl_list == i)\n",
    "        tmp_isNoisy = noise_list[class_lbl_list[i]]\n",
    "        tmp_isNoisy = torch.from_numpy(tmp_isNoisy)\n",
    "        class_isNoisy_list.append(tmp_isNoisy)\n",
    "\n",
    "        class_sin_score.append(loss_score[class_lbl_list[i]])\n",
    "        tmp_sort, tmp_idx = torch.sort(class_sin_score[i])\n",
    "\n",
    "        tmp_nc = torch.index_select(tmp_isNoisy, 0, tmp_idx[:frac])\n",
    "        \n",
    "        class_sel_list.append(tmp_nc)\n",
    "\n",
    "    return class_lbl_list, class_isNoisy_list, class_sin_score, class_sel_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "t_loss_list = torch.from_numpy(loss_list)\n",
    "topk_lbl_list, topk_isNoisy_list, topk_sin_list, topk_list = loss_topk(label_list, isNoisy_list, t_loss_list, 0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "class 0 top 50% clean/selected: 0.5534565916398714\n",
      "1377.0\n",
      "2488\n",
      "class 1 top 50% clean/selected: 0.5756979944946914\n",
      "1464.0\n",
      "2543\n",
      "class 2 top 50% clean/selected: 0.5213501423342822\n",
      "1282.0\n",
      "2459\n",
      "class 3 top 50% clean/selected: 0.5252485089463221\n",
      "1321.0\n",
      "2515\n",
      "class 4 top 50% clean/selected: 0.5234685759745425\n",
      "1316.0\n",
      "2514\n",
      "class 5 top 50% clean/selected: 0.5220851571826501\n",
      "1312.0\n",
      "2513\n",
      "class 6 top 50% clean/selected: 0.541335453100159\n",
      "1362.0\n",
      "2516\n",
      "class 7 top 50% clean/selected: 0.5359712230215827\n",
      "1341.0\n",
      "2502\n",
      "class 8 top 50% clean/selected: 0.5494728304947283\n",
      "1355.0\n",
      "2466\n",
      "class 9 top 50% clean/selected: 0.5384925433293026\n",
      "1336.0\n",
      "2481\n",
      "Total top 1% clean/selected: 0.5387046445573469\n",
      "24997\n",
      "13466.0\n"
     ]
    }
   ],
   "source": [
    "selected_data, selected_noise = 0, 0\n",
    "\n",
    "for i in range (0, len(topk_list)):\n",
    "    tmp_selected, tmp_noise = topk_list[i].shape[0], torch.sum(topk_list[i], 0)\n",
    "    \n",
    "    selected_data += tmp_selected\n",
    "    selected_noise += tmp_noise\n",
    "    print('class {} top {}% clean/selected: {}' \\\n",
    "          .format(i, round(100*tmp_selected / topk_sin_list[i].shape[0]), 1 - (tmp_noise / tmp_selected)))\n",
    "    print((tmp_selected - tmp_noise).item())\n",
    "    print(tmp_selected)\n",
    "    \n",
    "print('Total top {}% clean/selected: {}'.format(1, 1 - (selected_noise / selected_data)))\n",
    "print(selected_data)\n",
    "print((selected_data - selected_noise).item())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
