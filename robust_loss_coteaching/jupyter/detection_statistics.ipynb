{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "changed-yukon",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, sys\n",
    "os.chdir('../')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "confused-surfing",
   "metadata": {},
   "outputs": [],
   "source": [
    "import argparse\n",
    "import torch\n",
    "from tqdm import tqdm\n",
    "import data_loader.data_loaders as module_data\n",
    "import loss as module_loss\n",
    "import model.metric as module_metric\n",
    "import model.model as module_arch\n",
    "\n",
    "import easydict\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import sys\n",
    "import os\n",
    "import json\n",
    "import random\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt \n",
    "\n",
    "import data_loader.data_loaders as module_data\n",
    "import model.model as module_arch\n",
    "\n",
    "from selection.svd_classifier import *\n",
    "from selection.gmm import *\n",
    "from selection.util import *\n",
    "\n",
    "from utils.parse_config import ConfigParser\n",
    "from utils.util import *\n",
    "from utils.args import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "domestic-acquisition",
   "metadata": {},
   "outputs": [],
   "source": [
    "config_file = './hyperparams/multistep/config_cifar10_cce_rn34.json'\n",
    "with open(config_file, 'r') as f:\n",
    "    config = json.load(f)\n",
    "\n",
    "# resume_path = './rn34/multistep_asym_40_elr.pth'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "effective-bleeding",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = module_arch.resnet34(num_classes=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "listed-depression",
   "metadata": {},
   "outputs": [],
   "source": [
    "parse, config = make_parse(resume_path, config, 0.4, True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "structured-column",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n",
      "##############\n",
      "[3 2 1 1 3 0 0 7 6 5]\n",
      "[3 2 1 1 3 2 2 7 6 5]\n",
      "Train: 50000 Val: 0\n"
     ]
    }
   ],
   "source": [
    "data_loader = getattr(module_data, config['data_loader']['type'])(\n",
    "    config['data_loader']['args']['data_dir'],\n",
    "    batch_size= 100,\n",
    "    shuffle=config['data_loader']['args']['shuffle'],\n",
    "    validation_split=0.0,\n",
    "    num_batches=config['data_loader']['args']['num_batches'],\n",
    "    training=True,\n",
    "    num_workers=config['data_loader']['args']['num_workers'],\n",
    "    pin_memory=config['data_loader']['args']['pin_memory'],\n",
    "    config=config\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "peripheral-thesis",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 500/500 [00:16<00:00, 22.15it/s]\n",
      "100%|██████████| 10/10 [00:15<00:00,  1.37s/it]\n",
      "100%|██████████| 50000/50000 [00:00<00:00, 345861.51it/s]\n",
      "100%|██████████| 500/500 [00:02<00:00, 167.56it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Selected samples: 31607 \n",
      "Precision: 0.8145 \n",
      "Recall: 0.6314 \n",
      "Specificity: 0.3645\n",
      "Accuracy: 0.5821 \n",
      "Fraction of clean samples/selected samples: 0.8145\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(31607, 0.8145, 0.6314, 0.3645, 0.5821)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "selected, precision, recall, specificity, accuracy = extract_cleanidx(model, data_loader, parse)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "dominican-medline",
   "metadata": {},
   "outputs": [],
   "source": [
    "pathlist = os.listdir('./checkpoint/rn34/')\n",
    "pathlist = [path for path in pathlist if ('eigen' not in path) and ('kmeans' not in path) and ('c100') not in path]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "oriental-contact",
   "metadata": {},
   "outputs": [],
   "source": [
    "logcolumns = ['name', 'selected', 'precision', 'recall', 'specificity', 'accuracy']\n",
    "log_pd = pd.DataFrame(np.zeros([1, len(logcolumns)]), columns = logcolumns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "pleasant-particular",
   "metadata": {},
   "outputs": [],
   "source": [
    "log_pd.loc[0] = [pathlist[0], 31607, 0.8145, 0.6314, 0.3645, 0.5821]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "portuguese-edward",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>name</th>\n",
       "      <th>selected</th>\n",
       "      <th>precision</th>\n",
       "      <th>recall</th>\n",
       "      <th>specificity</th>\n",
       "      <th>accuracy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>multistep_sym_80_elr.pth</td>\n",
       "      <td>31607.0</td>\n",
       "      <td>0.8145</td>\n",
       "      <td>0.6314</td>\n",
       "      <td>0.3645</td>\n",
       "      <td>0.5821</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                       name  selected  precision  recall  specificity  \\\n",
       "0  multistep_sym_80_elr.pth   31607.0     0.8145  0.6314       0.3645   \n",
       "\n",
       "   accuracy  \n",
       "0    0.5821  "
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "log_pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "moderate-independence",
   "metadata": {},
   "outputs": [],
   "source": [
    "def decode(path):\n",
    "    items = path.split('_')\n",
    "    noisetype = True if items[1]=='asym' else False\n",
    "    noiserate = float(items[2]) * 0.01\n",
    "    \n",
    "    return noisetype, noiserate, items[3].split('.')[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "focused-press",
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_parse(resume_path, config, noise_rate, noisetype):\n",
    "    parse = easydict.EasyDict({\n",
    "    \"load_name\" : resume_path,\n",
    "    \"reinit\": False,\n",
    "    \"distill_mode\": 'kmeans'\n",
    "    })\n",
    "    \n",
    "    config['trainer']['percent'] = noise_rate\n",
    "    config['trainer']['asym'] = noisetype\n",
    "    \n",
    "    return parse, config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "proud-banana",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_cleanidx(teacher, data_loader, parse, print_statistics = True):\n",
    "    teacher.load_state_dict(torch.load('./checkpoint/' + parse.load_name)['state_dict'])\n",
    "    teacher = teacher.cuda()\n",
    "\n",
    "    if not parse.reinit: teacher.load_state_dict(torch.load('./checkpoint/' + parse.load_name)['state_dict'])\n",
    "    for params in teacher.parameters(): params.requires_grad = False\n",
    "    \n",
    "    features, labels = get_features(teacher, data_loader)\n",
    "    clean_labels = fine(current_features=features, current_labels=labels, fit = parse.distill_mode)\n",
    "    \n",
    "    if print_statistics: \n",
    "        selected, precision, recall, specificity, accuracy = return_statistics(data_loader, clean_labels, datanum=len(labels))\n",
    "    \n",
    "    return selected, precision, recall, specificity, accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "innocent-future",
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_pd_list(root, config, log_filename):\n",
    "    # load checkpoint path\n",
    "    pathlist = os.listdir(root)\n",
    "    pathlist = [path for path in pathlist if ('eigen' not in path) and ('kmeans' not in path) and ('c100') not in path]\n",
    "    \n",
    "    # load original dataloader\n",
    "    data_loader = getattr(module_data, config['data_loader']['type'])(\n",
    "    config['data_loader']['args']['data_dir'],\n",
    "    batch_size= 100,\n",
    "    shuffle=config['data_loader']['args']['shuffle'],\n",
    "    validation_split=0.0,\n",
    "    num_batches=config['data_loader']['args']['num_batches'],\n",
    "    training=True,\n",
    "    num_workers=config['data_loader']['args']['num_workers'],\n",
    "    pin_memory=config['data_loader']['args']['pin_memory'],\n",
    "    config=config)\n",
    "    \n",
    "    # initialize model\n",
    "    model = module_arch.resnet34(num_classes=10)\n",
    "    \n",
    "    # make pandas file\n",
    "    logcolumns = ['noisetype', 'noiserate', 'lossfunction', 'selected', 'precision', 'recall', 'specificity', 'accuracy']\n",
    "    log_pd = pd.DataFrame(np.zeros([len(pathlist), len(logcolumns)]), columns = logcolumns)\n",
    "    \n",
    "    # write pandas file\n",
    "    for i in range(len(pathlist)):\n",
    "        noisetype, noiserate, lossfunction = decode(pathlist[i])\n",
    "        parse, config = make_parse('./rn34/' + pathlist[i], config, noiserate, noisetype)\n",
    "        selected, precision, recall, specificity, accuracy = extract_cleanidx(model, data_loader, parse)\n",
    "        log_pd.loc[i] = [str(noisetype), str(noiserate), lossfunction, pathlist[i].split('.')[0], selected, precision, recall, specificity, accuracy]\n",
    "        log_pd.to_csv(log_filename)\n",
    "        \n",
    "    return log_pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "rational-patient",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n",
      "##############\n",
      "[8 9 1 9 3 8 3 7 3 5]\n",
      "[3 2 1 1 3 2 2 7 6 5]\n",
      "Train: 50000 Val: 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 500/500 [01:31<00:00,  5.39it/s]\n",
      " 20%|██        | 2/10 [00:03<00:12,  1.52s/it]"
     ]
    }
   ],
   "source": [
    "make_pd_list(root = './checkpoint/rn34/', config=config, log_filename = 'pretrained_statistics.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "loved-triangle",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
