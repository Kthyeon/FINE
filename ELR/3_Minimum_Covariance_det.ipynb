{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import argparse\n",
    "import torch\n",
    "import sys\n",
    "import os\n",
    "import json\n",
    "import random\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import pickle\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt \n",
    "import sklearn.covariance\n",
    "import scipy\n",
    "import pdb\n",
    "\n",
    "import data_loader.data_loaders as module_data\n",
    "import loss as module_loss\n",
    "import model.metric as module_metric\n",
    "import model.model as module_arch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import model.model as module_arch\n",
    "\n",
    "from sklearn import metrics\n",
    "from sklearn import cluster\n",
    "from tqdm import tqdm\n",
    "from torch.autograd import Variable\n",
    "from parse_config import ConfigParser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "config_file = './hyperparams/multistep/config_cifar10_gce_rn34.json'\n",
    "with open(config_file, 'r') as f:\n",
    "    config = json.load(f)\n",
    "    \n",
    "device = torch.device('cuda:0')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'name': 'cifar10_resnet34_multistep',\n",
       " 'n_gpu': 1,\n",
       " 'seed': 123,\n",
       " 'arch': {'type': 'resnet34', 'args': {'num_classes': 10}},\n",
       " 'num_classes': 10,\n",
       " 'data_loader': {'type': 'CIFAR10DataLoader',\n",
       "  'args': {'data_dir': './dir_to_data',\n",
       "   'batch_size': 128,\n",
       "   'shuffle': True,\n",
       "   'num_batches': 0,\n",
       "   'validation_split': 0,\n",
       "   'num_workers': 8,\n",
       "   'pin_memory': True}},\n",
       " 'optimizer': {'type': 'SGD',\n",
       "  'args': {'lr': 0.02, 'momentum': 0.9, 'weight_decay': 0.001}},\n",
       " 'train_loss': {'type': 'GCELoss',\n",
       "  'args': {'q': 0.7, 'k': 0.5, 'truncated': False}},\n",
       " 'val_loss': 'CrossEntropyLoss',\n",
       " 'metrics': ['my_metric', 'my_metric2'],\n",
       " 'lr_scheduler': {'type': 'MultiStepLR',\n",
       "  'args': {'milestones': [40, 80], 'gamma': 0.01}},\n",
       " 'trainer': {'epochs': 120,\n",
       "  'warmup': 0,\n",
       "  'save_dir': 'saved/',\n",
       "  'save_period': 1,\n",
       "  'verbosity': 2,\n",
       "  'label_dir': 'saved/',\n",
       "  'monitor': 'max test_my_metric',\n",
       "  'early_stop': 2000,\n",
       "  'tensorboard': False,\n",
       "  'mlflow': True,\n",
       "  '_percent': 'Percentage of noise',\n",
       "  'percent': 0.4,\n",
       "  '_begin': 'When to begin updating labels',\n",
       "  'begin': 0,\n",
       "  '_asym': 'symmetric noise if false',\n",
       "  'asym': False}}"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "config['trainer']['percent'] = 0.4\n",
    "config['trainer']['asym'] = False\n",
    "config['train_loss']['type'] = 'GCELoss'\n",
    "config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "resume_path = './saved/models/cifar10/resnet34/MultiStepLR/GCELoss/sym/40/model_best123.pth'\n",
    "base_model = getattr(module_arch, config[\"arch\"]['type'])()\n",
    "checkpoint = torch.load(resume_path)\n",
    "state_dict = checkpoint['state_dict']\n",
    "base_model.load_state_dict(state_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n",
      "Train: 50000 Val: 0\n"
     ]
    }
   ],
   "source": [
    "# set seed\n",
    "random.seed(config['seed'])\n",
    "torch.manual_seed(config['seed'])\n",
    "torch.cuda.manual_seed_all(config['seed'])\n",
    "torch.backends.cudnn.deterministic = True\n",
    "np.random.seed(config['seed'])\n",
    "\n",
    "data_loader = getattr(module_data, config['data_loader']['type'])(\n",
    "    config['data_loader']['args']['data_dir'],\n",
    "    batch_size= 100,\n",
    "    shuffle=config['data_loader']['args']['shuffle'],\n",
    "    validation_split=0.0,\n",
    "    num_batches=config['data_loader']['args']['num_batches'],\n",
    "    training=True,\n",
    "    num_workers=config['data_loader']['args']['num_workers'],\n",
    "    pin_memory=config['data_loader']['args']['pin_memory'],\n",
    "    config=config\n",
    ")\n",
    "\n",
    "if hasattr(data_loader.dataset, 'num_raw_example'):\n",
    "    num_examp = data_loader.dataset.num_raw_example\n",
    "else:\n",
    "    num_examp = len(data_loader.dataset)\n",
    "\n",
    "\n",
    "criterion = getattr(module_loss, 'GCELoss')(q=config['train_loss']['args']['q'],\n",
    "                                            k=config['train_loss']['args']['k'],\n",
    "                                            trainset_size=num_examp,\n",
    "                                            truncated=config['train_loss']['args']['truncated'])\n",
    "\n",
    "# criterion = getattr(module_loss, 'GCELoss')(q=config['train_loss']['args']['q'],\n",
    "#                                                      k=config['train_loss']['args']['k'],\n",
    "#                                                      truncated=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Base Resnet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Represent(nn.Module):\n",
    "    def __init__(self, base_model):\n",
    "        super(Represent, self).__init__()\n",
    "        self.conv1 = base_model.conv1\n",
    "        self.bn1 = base_model.bn1\n",
    "        self.layer1 = base_model.layer1\n",
    "        self.layer2 = base_model.layer2\n",
    "        self.layer3 = base_model.layer3\n",
    "        self.layer4 = base_model.layer4\n",
    "        self.linear = base_model.linear\n",
    "        \n",
    "        \n",
    "    def forward(self, x):\n",
    "        out = F.relu(self.bn1(self.conv1(x)))\n",
    "        out = self.layer1(out)\n",
    "        out = self.layer2(out)\n",
    "        out = self.layer3(out)\n",
    "        out = self.layer4(out)\n",
    "        out = F.avg_pool2d(out, 4)\n",
    "        y = out.view(out.size(0), -1)\n",
    "        \n",
    "        return y\n",
    "    \n",
    "    #Feature Extractting\n",
    "    def feature_list(self, x):\n",
    "        output_list = []\n",
    "        out = F.relu(self.bn1(self.conv1(x)))\n",
    "        for name, module in self.layer1._modules.items():\n",
    "            out = module(out)\n",
    "        for name, module in self.layer2._modules.items():\n",
    "            out = module(out)\n",
    "        for name, module in self.layer3._modules.items():\n",
    "            out = module(out)\n",
    "        for name, module in self.layer4._modules.items():\n",
    "            out = module(out)\n",
    "            output_list.append(out)\n",
    "        out = F.avg_pool2d(out, 4)\n",
    "        out = out.view(out.size(0), -1)\n",
    "        y = self.linear(out)\n",
    "        return y, output_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Represent(base_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "isNoisy_list = np.empty((0,))\n",
    "isFalse_list = np.empty((0,))\n",
    "label_list = np.empty((0,))\n",
    "gt_list = np.empty((0,))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "noisy_label = data_loader.dataset.train_labels\n",
    "gt_label = data_loader.dataset.train_labels_gt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Noise rate:  0.3606\n"
     ]
    }
   ],
   "source": [
    "#Noise rate check\n",
    "tmp = 0\n",
    "for i in range(len(gt_label)):\n",
    "    if noisy_label[i] != gt_label[i]:\n",
    "        tmp += 1\n",
    "print('Noise rate: ', tmp/len(gt_label))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# FNAES / K-means"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 500/500 [00:29<00:00, 16.79it/s]\n"
     ]
    }
   ],
   "source": [
    "isNoisy_list = np.empty((0,))\n",
    "isFalse_list = np.empty((0,))\n",
    "label_list = np.empty((0,))\n",
    "gt_list = np.empty((0,))\n",
    "conf_list = np.empty((0,))\n",
    "loss_list = np.empty((0,))\n",
    "\n",
    "model.eval()\n",
    "model.to(device)\n",
    "\n",
    "with tqdm(data_loader) as progress:\n",
    "    for batch_idx, (data, label, index, label_gt) in enumerate(progress):\n",
    "        data = data.to(device)\n",
    "        label, label_gt = label.long().to(device), label_gt.long().to(device)\n",
    "        output = model(data)\n",
    "        _,prediction = base_model(data)\n",
    "        loss = torch.nn.CrossEntropyLoss(reduction='none')(prediction, label)\n",
    "        confidence, _ = torch.max(torch.nn.functional.softmax(prediction, dim=1), dim=1)\n",
    "        isNoisy = label != label_gt\n",
    "        \n",
    "        gt_list = np.concatenate((gt_list, label_gt.cpu()))\n",
    "        label_list = np.concatenate((label_list, label.cpu()))\n",
    "        isNoisy_list = np.concatenate((isNoisy_list, isNoisy.cpu()))\n",
    "        conf_list = np.concatenate((conf_list, confidence.detach().cpu()))\n",
    "        loss_list = np.concatenate((loss_list, loss.detach().cpu()))\n",
    "        if batch_idx == 0:\n",
    "            out_list = output.detach().cpu()\n",
    "        else:\n",
    "            out_list = np.concatenate((out_list, output.detach().cpu()), axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.3606"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#check noisy rate\n",
    "isNoisy_list.sum()/len(data_loader.dataset.targets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_singular_value_vector(label_list, out_list):\n",
    "    \n",
    "    singular_dict = {}\n",
    "    v_ortho_dict = {}\n",
    "    \n",
    "    for index in np.unique(label_list):\n",
    "        u, s, v = np.linalg.svd(out_list[label_list==index])\n",
    "        singular_dict[index] = s[0] / s[1]\n",
    "        v_ortho_dict[index] = torch.from_numpy(v[:2])\n",
    "\n",
    "    return singular_dict, v_ortho_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def singular_label(v_ortho_dict, model_represents, label):\n",
    "    \n",
    "    model_represents = torch.from_numpy(model_represents).to(device)\n",
    "    sing_lbl = torch.zeros(model_represents.shape[0]) \n",
    "    sin_score_lbl = torch.zeros(model_represents.shape[0])\n",
    "    \n",
    "    for i, data in enumerate(model_represents):\n",
    "        sin_score_lbl[i] = torch.dot(v_ortho_dict[label[i]][0], data).abs() - torch.dot(v_ortho_dict[label[i]][1], data).abs()\n",
    "        if torch.dot(v_ortho_dict[label[i]][0], data).abs() < torch.dot(v_ortho_dict[label[i]][1], data).abs():\n",
    "            sing_lbl[i] = 1\n",
    "        \n",
    "    return sing_lbl, sin_score_lbl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "singular_dict, v_ortho_dict = get_singular_value_vector(label_list, out_list)\n",
    "\n",
    "for key in v_ortho_dict.keys():\n",
    "    v_ortho_dict[key] = v_ortho_dict[key].to(device)\n",
    "\n",
    "sing_lbl, sin_score_lbl = singular_label(v_ortho_dict, out_list, label_list)\n",
    "kmeans = cluster.KMeans(n_clusters=2, random_state=0).fit(loss_list.reshape(-1,1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "50000"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(isNoisy_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[32274.0, 0.9538, 0.93771, 0.97438, 0.96116, 0.9652]"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def return_statistics(isNoisy_list, predict):\n",
    "    r_stats = []\n",
    "    \n",
    "    tp = isNoisy_list[predict==1].sum()\n",
    "    tn = (isNoisy_list[predict==0]==0).sum()\n",
    "    fp = (isNoisy_list.shape - isNoisy_list.sum()) - tn\n",
    "    fn = isNoisy_list.sum() - tp\n",
    "    \n",
    "    precision = tp / (tp + fp)\n",
    "    recall = tp / (tp + fn)\n",
    "    specificity = tn / (tn + fp)\n",
    "    accuracy = (tp + tn) / (tp + tn + fp + fn)\n",
    "    sel_samples = fn + tn\n",
    "    frac_clean = tn / (fn + tn)\n",
    "    \n",
    "    r_stats.extend([sel_samples, round(precision[0], 5), round(recall, 5), round(specificity[0], 5), round(accuracy[0], 5), round(frac_clean, 5)])\n",
    "    \n",
    "    return r_stats\n",
    "\n",
    "return_statistics(isNoisy_list, 1-kmeans.labels_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "def stat_summary(name, stat_list):\n",
    "    print('Dataset: {}, Net: {}, Noise{}_{}, Loss: {}'\n",
    "      .format(config['data_loader']['type'], config['arch']['type'], config['trainer']['asym'], config['trainer']['percent'], config['train_loss']['type']))\n",
    "    \n",
    "    print(\"=\"*50, name , \"=\"*50)\n",
    "\n",
    "    print('Selected samples by {}: {} \\nPrecision: {} \\nRecall: {} \\nSpecificity: {}\\nAccuracy: {} \\nFraction of clean samples/selected samples: {}'\n",
    "                  .format(name, stat_list[0], stat_list[1], stat_list[2], stat_list[3], stat_list[4], stat_list[5]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset: CIFAR10DataLoader, Net: resnet34, NoiseFalse_0.4, Loss: GCELoss\n",
      "================================================== K-means Clustering ==================================================\n",
      "Selected samples by K-means Clustering: 32274.0 \n",
      "Precision: 0.9538 \n",
      "Recall: 0.93771 \n",
      "Specificity: 0.97438\n",
      "Accuracy: 0.96116 \n",
      "Fraction of clean samples/selected samples: 0.9652\n"
     ]
    }
   ],
   "source": [
    "k_mean_stat = return_statistics(isNoisy_list, 1-kmeans.labels_)\n",
    "stat_summary('K-means Clustering', k_mean_stat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset: CIFAR10DataLoader, Net: resnet34, NoiseFalse_0.4, Loss: GCELoss\n",
      "================================================== FNAES ==================================================\n",
      "Selected samples by FNAES: 32143.0 \n",
      "Precision: 0.94994 \n",
      "Recall: 0.94082 \n",
      "Specificity: 0.97204\n",
      "Accuracy: 0.96078 \n",
      "Fraction of clean samples/selected samples: 0.9668\n"
     ]
    }
   ],
   "source": [
    "fnaes_stat = return_statistics(isNoisy_list, sing_lbl)\n",
    "stat_summary('FNAES', fnaes_stat)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MCD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "#get raw data\n",
    "\n",
    "for batch_idx, (data, target, index, label_gt) in enumerate(data_loader):\n",
    "    data, target, label_gt = data.cuda(), target.cuda(), label_gt.cuda()\n",
    "    if batch_idx == 0:\n",
    "        total_data = data\n",
    "        total_target = target\n",
    "        total_label_gt = label_gt\n",
    "    else:\n",
    "        total_data = torch.cat((total_data, data), 0)\n",
    "        total_target = torch.cat((total_target, target), 0)\n",
    "        total_label_gt = torch.cat((total_label_gt, label_gt), 0)\n",
    "        \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Noise rate:  0.3606\n"
     ]
    }
   ],
   "source": [
    "#Noise rate check\n",
    "tmp = 0\n",
    "for i in range(len(total_label_gt)):\n",
    "    if total_target[i] != total_label_gt[i]:\n",
    "        tmp += 1\n",
    "print('Noise rate: ', tmp/len(total_label_gt))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-51-2d1080d009e6>:3: UserWarning: volatile was removed and now has no effect. Use `with torch.no_grad():` instead.\n",
      "  temp_x = Variable(temp_x, volatile=True)\n",
      "<ipython-input-51-2d1080d009e6>:12: UserWarning: volatile was removed and now has no effect. Use `with torch.no_grad():` instead.\n",
      "  data = Variable(data, volatile=True)\n"
     ]
    }
   ],
   "source": [
    "model.eval()\n",
    "temp_x = torch.rand(2,3,32,32).cuda()\n",
    "temp_x = Variable(temp_x, volatile=True)\n",
    "temp_list = model.feature_list(temp_x)[1]\n",
    "num_output = len(temp_list) # Number of layers that extracts feature\n",
    "total_final_feature = [0]*num_output #Extracted Features\n",
    "total = 0\n",
    "batch_size = 100\n",
    "\n",
    "for data_index in range(int(np.floor(total_data.size(0)/batch_size))):\n",
    "    data = total_data[total : total + batch_size]\n",
    "    data = Variable(data, volatile=True)\n",
    "\n",
    "    _, out_features = model.feature_list(data)\n",
    "    for i in range(num_output):\n",
    "        out_features[i] = out_features[i].view(out_features[i].size(0), out_features[i].size(1), -1)\n",
    "        out_features[i] = torch.mean(out_features[i].data, 2)\n",
    "        if total == 0:\n",
    "            total_final_feature[i] = out_features[i].cpu().clone()\n",
    "        else:\n",
    "            total_final_feature[i] = torch.cat((total_final_feature[i], out_features[i].cpu().clone()), 0)\n",
    "    total += batch_size\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([50000, 512])"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "total_final_feature[0].shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Random Initailzation of samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Random Sample Mean\n",
    "def random_sample_mean(feature, total_label, num_classes):\n",
    "    \n",
    "    group_lasso = sklearn.covariance.EmpiricalCovariance(assume_centered = False)    \n",
    "    new_feature, fraction_list = [], []\n",
    "    frac = 0.7\n",
    "    sample_mean_per_class = torch.Tensor(num_classes, feature.size(1)).fill_(0).cuda()\n",
    "    total_label = total_label.cuda()\n",
    "\n",
    "    \n",
    "    total_selected_list = []\n",
    "    for i in range(num_classes):\n",
    "        index_list = total_label.eq(i)\n",
    "        temp_feature = feature[index_list.nonzero(), :]\n",
    "        temp_feature = temp_feature.view(temp_feature.size(0), -1)\n",
    "        shuffler_idx = torch.randperm(temp_feature.size(0))\n",
    "        index = shuffler_idx[:int(temp_feature.size(0)*frac)]\n",
    "        fraction_list.append(int(temp_feature.size(0)*frac))\n",
    "        total_selected_list.append(index_list.nonzero()[index.cuda()])\n",
    "\n",
    "        selected_feature = torch.index_select(temp_feature, 0, index.cuda())\n",
    "        new_feature.append(selected_feature)\n",
    "        sample_mean_per_class[i].copy_(torch.mean(selected_feature, 0))\n",
    "    \n",
    "    total_covariance = 0\n",
    "    for i in range(num_classes):\n",
    "        flag = 0\n",
    "        X = 0\n",
    "        for j in range(fraction_list[i]):\n",
    "            temp_feature = new_feature[i][j]\n",
    "            temp_feature = temp_feature - sample_mean_per_class[i]\n",
    "            temp_feature = temp_feature.view(-1,1)\n",
    "            if flag  == 0:\n",
    "                X = temp_feature.transpose(0,1)\n",
    "                flag = 1\n",
    "            else:\n",
    "                X = torch.cat((X,temp_feature.transpose(0,1)),0)\n",
    "            # find inverse            \n",
    "        group_lasso.fit(X.cpu().numpy())\n",
    "        inv_sample_conv = group_lasso.covariance_\n",
    "        inv_sample_conv = torch.from_numpy(inv_sample_conv).float().cuda()\n",
    "        if i == 0:\n",
    "            total_covariance = inv_sample_conv*fraction_list[i]\n",
    "        else:\n",
    "            total_covariance += inv_sample_conv*fraction_list[i]\n",
    "        total_covariance = total_covariance/sum(fraction_list)\n",
    "    new_precision = scipy.linalg.pinvh(total_covariance.cpu().numpy())\n",
    "    new_precision = torch.from_numpy(new_precision).float().cuda()\n",
    "    \n",
    "    return sample_mean_per_class, new_precision, total_selected_list\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Random Sample Mean\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-53-3867dc7d9129>:14: UserWarning: This overload of nonzero is deprecated:\n",
      "\tnonzero()\n",
      "Consider using one of the following signatures instead:\n",
      "\tnonzero(*, bool as_tuple) (Triggered internally at  /opt/conda/conda-bld/pytorch_1607370172916/work/torch/csrc/utils/python_arg_parser.cpp:882.)\n",
      "  temp_feature = feature[index_list.nonzero(), :]\n"
     ]
    }
   ],
   "source": [
    "print('Random Sample Mean')\n",
    "sample_mean_list, sample_precision_list = [], []\n",
    "total_label_list = [total_target for i in range(num_output)]\n",
    "\n",
    "for index in range(num_output):\n",
    "    sample_mean, sample_precision, _ = random_sample_mean(total_final_feature[index].cuda(), total_label_list[index].cuda(), config['num_classes'])\n",
    "    sample_mean_list.append(sample_mean)\n",
    "    sample_precision_list.append(sample_precision)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([512])"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#mean for each entity corresponding to feature vector\n",
    "sample_mean_list[2][1].shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Minimum Covariance determinent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "#MCD single\n",
    "def MCD_single(feature, sample_mean, inverse_covariance):\n",
    "    group_lasso = sklearn.covariance.EmpiricalCovariance(assume_centered=False)\n",
    "    temp_batch = 100\n",
    "    total, mahalanobis_score = 0, 0\n",
    "    frac = 0.7 #fraction N_c+d+1 / 2\n",
    "    for data_index in range(int(np.ceil(feature.size(0)/temp_batch))):\n",
    "        temp_feature = feature[total : total + temp_batch].cuda()        \n",
    "        gaussian_score = 0\n",
    "        batch_sample_mean = sample_mean\n",
    "        zero_f = temp_feature - batch_sample_mean\n",
    "        term_gau = -0.5*torch.mm(torch.mm(zero_f, inverse_covariance), zero_f.t()).diag()\n",
    "        # concat data\n",
    "        if total == 0:\n",
    "            mahalanobis_score = term_gau.view(-1,1)\n",
    "        else:\n",
    "            mahalanobis_score = torch.cat((mahalanobis_score, term_gau.view(-1,1)), 0)\n",
    "        total += temp_batch\n",
    "        \n",
    "    mahalanobis_score = mahalanobis_score.view(-1)\n",
    "    feature = feature.view(feature.size(0), -1)\n",
    "    _, selected_idx = torch.topk(mahalanobis_score, int(feature.size(0)*frac))\n",
    "    selected_feature = torch.index_select(feature, 0, selected_idx.cuda())\n",
    "    new_sample_mean = torch.mean(selected_feature, 0)\n",
    "    \n",
    "    # compute covariance matrix\n",
    "    X = 0\n",
    "    flag = 0\n",
    "    for j in range(selected_feature.size(0)):\n",
    "        temp_feature = selected_feature[j]\n",
    "        temp_feature = temp_feature - new_sample_mean\n",
    "        temp_feature = temp_feature.view(-1,1)\n",
    "        if flag  == 0:\n",
    "            X = temp_feature.transpose(0,1)\n",
    "            flag = 1\n",
    "        else:\n",
    "            X = torch.cat((X, temp_feature.transpose(0,1)),0)\n",
    "    # find inverse            \n",
    "    group_lasso.fit(X.cpu().numpy())\n",
    "    new_sample_cov = group_lasso.covariance_\n",
    "    \n",
    "    return new_sample_mean, new_sample_cov, selected_idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Single MCD and merge the parameters\n",
      "torch.Size([5035, 1, 512])\n",
      "selcted index for class 0 : torch.Size([3524])\n",
      "torch.Size([4992, 1, 512])\n",
      "selcted index for class 1 : torch.Size([3494])\n",
      "torch.Size([4882, 1, 512])\n",
      "selcted index for class 2 : torch.Size([3417])\n",
      "torch.Size([4955, 1, 512])\n",
      "selcted index for class 3 : torch.Size([3468])\n",
      "torch.Size([5044, 1, 512])\n",
      "selcted index for class 4 : torch.Size([3530])\n",
      "torch.Size([4969, 1, 512])\n",
      "selcted index for class 5 : torch.Size([3478])\n",
      "torch.Size([4988, 1, 512])\n",
      "selcted index for class 6 : torch.Size([3491])\n",
      "torch.Size([5040, 1, 512])\n",
      "selcted index for class 7 : torch.Size([3528])\n",
      "torch.Size([4995, 1, 512])\n",
      "selcted index for class 8 : torch.Size([3496])\n",
      "torch.Size([5100, 1, 512])\n",
      "selcted index for class 9 : torch.Size([3570])\n",
      "torch.Size([5035, 1, 512])\n",
      "selcted index for class 0 : torch.Size([3524])\n",
      "torch.Size([4992, 1, 512])\n",
      "selcted index for class 1 : torch.Size([3494])\n",
      "torch.Size([4882, 1, 512])\n",
      "selcted index for class 2 : torch.Size([3417])\n",
      "torch.Size([4955, 1, 512])\n",
      "selcted index for class 3 : torch.Size([3468])\n",
      "torch.Size([5044, 1, 512])\n",
      "selcted index for class 4 : torch.Size([3530])\n",
      "torch.Size([4969, 1, 512])\n",
      "selcted index for class 5 : torch.Size([3478])\n",
      "torch.Size([4988, 1, 512])\n",
      "selcted index for class 6 : torch.Size([3491])\n",
      "torch.Size([5040, 1, 512])\n",
      "selcted index for class 7 : torch.Size([3528])\n",
      "torch.Size([4995, 1, 512])\n",
      "selcted index for class 8 : torch.Size([3496])\n",
      "torch.Size([5100, 1, 512])\n",
      "selcted index for class 9 : torch.Size([3570])\n",
      "torch.Size([5035, 1, 512])\n",
      "selcted index for class 0 : torch.Size([3524])\n",
      "torch.Size([4992, 1, 512])\n",
      "selcted index for class 1 : torch.Size([3494])\n",
      "torch.Size([4882, 1, 512])\n",
      "selcted index for class 2 : torch.Size([3417])\n",
      "torch.Size([4955, 1, 512])\n",
      "selcted index for class 3 : torch.Size([3468])\n",
      "torch.Size([5044, 1, 512])\n",
      "selcted index for class 4 : torch.Size([3530])\n",
      "torch.Size([4969, 1, 512])\n",
      "selcted index for class 5 : torch.Size([3478])\n",
      "torch.Size([4988, 1, 512])\n",
      "selcted index for class 6 : torch.Size([3491])\n",
      "torch.Size([5040, 1, 512])\n",
      "selcted index for class 7 : torch.Size([3528])\n",
      "torch.Size([4995, 1, 512])\n",
      "selcted index for class 8 : torch.Size([3496])\n",
      "torch.Size([5100, 1, 512])\n",
      "selcted index for class 9 : torch.Size([3570])\n"
     ]
    }
   ],
   "source": [
    "print('Single MCD and merge the parameters')\n",
    "new_sample_mean_list = []\n",
    "new_sample_precision_list = []\n",
    "selected_feature = []\n",
    "layer_selected_index = []\n",
    "for index in range(num_output):\n",
    "    tmp_selected_idx = []\n",
    "\n",
    "    new_sample_mean = torch.Tensor(config['num_classes'], total_final_feature[index].size(1)).fill_(0).cuda()\n",
    "    new_covariance = 0\n",
    "    for i in range(config['num_classes']):\n",
    "        index_list = total_label_list[index].eq(i)\n",
    "        temp_feature = total_final_feature[index][index_list.nonzero(), :]\n",
    "        tmp_idx_list = index_list.nonzero().view(-1).detach().cpu()\n",
    "        print(temp_feature.shape)\n",
    "        temp_feature = temp_feature.view(temp_feature.size(0), -1)\n",
    "        temp_mean, temp_cov, tmp_idx = MCD_single(temp_feature.cuda(), sample_mean_list[index][i], sample_precision_list[index])\n",
    "        print('selcted index for class', i, ':', tmp_idx.shape)\n",
    "        new_sample_mean[i].copy_(temp_mean)\n",
    "        tmp_real_idx = tmp_idx_list[tmp_idx.detach().cpu()]\n",
    "        tmp_selected_idx.extend(tmp_real_idx.tolist())\n",
    "\n",
    "\n",
    "        if i  == 0:\n",
    "            new_covariance = temp_feature.size(0)*temp_cov\n",
    "        else:\n",
    "            new_covariance += temp_feature.size(0)*temp_cov\n",
    "        \n",
    "    layer_selected_index.append(tmp_selected_idx)\n",
    "            \n",
    "    new_covariance = new_covariance / total_final_feature[index].size(0)\n",
    "    new_precision = scipy.linalg.pinvh(new_covariance)\n",
    "    new_precision = torch.from_numpy(new_precision).float().cuda()\n",
    "    new_sample_mean_list.append(new_sample_mean)\n",
    "    new_sample_precision_list.append(new_precision)\n",
    "\n",
    "G_soft_list = []\n",
    "target_mean = new_sample_mean_list \n",
    "target_precision = new_sample_precision_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3\n",
      "34996\n",
      "tensor([13872, 35878,  3958,  ..., 44155,  9615, 41085])\n"
     ]
    }
   ],
   "source": [
    "new_sample_mean_list[0].shape\n",
    "\n",
    "print(len(new_sample_mean_list))\n",
    "print(len(tmp_selected_idx))\n",
    "print(tmp_real_idx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "34996"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(layer_selected_index[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "34996\n",
      "34996\n",
      "34996\n"
     ]
    }
   ],
   "source": [
    "layer4_0 = layer_selected_index[0]\n",
    "layer4_1 = layer_selected_index[1]\n",
    "layer4_2 = layer_selected_index[2]\n",
    "\n",
    "print(len(layer4_0))\n",
    "print(len(layer4_1))\n",
    "print(len(layer4_2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "0\n",
      "0\n"
     ]
    }
   ],
   "source": [
    "#Check for same index\n",
    "import collections\n",
    "for i in range(len(layer_selected_index)):\n",
    "    print(len([item for item, count in collections.Counter(layer_selected_index[i]).items() if count > 1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Selected data Difference \n",
      "4_0 vs 4_1: 32036 \n",
      "4_1 vs 4_2: 33097 \n",
      "4_0 vs 4_2: 30842\n"
     ]
    }
   ],
   "source": [
    "print('Selected data Difference \\n4_0 vs 4_1: {} \\n4_1 vs 4_2: {} \\n4_0 vs 4_2: {}'\n",
    "      .format(len(set(layer4_0) & set(layer4_1)), len(set(layer4_1) & set(layer4_2)), len(set(layer4_0) & set(layer4_2))))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [],
   "source": [
    "saved_path = './saved' \n",
    "tmp_asym = 'asym' if config['trainer']['asym'] else 'sym'\n",
    "data_name, net_fam = config['name'].split('_')[0], config['name'].split('_')[1]\n",
    "\n",
    "if not os.path.isdir(saved_path):\n",
    "    os.mkdir(saved_path)\n",
    "next_path = os.path.join(saved_path, 'mahalanobis')\n",
    "if not os.path.isdir(next_path):\n",
    "    os.mkdir(next_path)\n",
    "next_path = os.path.join(next_path, data_name)\n",
    "if not os.path.isdir(next_path):\n",
    "    os.mkdir(next_path)\n",
    "next_path = os.path.join(next_path, net_fam)\n",
    "if not os.path.isdir(next_path):\n",
    "    os.mkdir(next_path)\n",
    "next_path = os.path.join(next_path, config['lr_scheduler']['type'])\n",
    "if not os.path.isdir(next_path):\n",
    "    os.mkdir(next_path)\n",
    "next_path = os.path.join(next_path, config['train_loss']['type'])\n",
    "if not os.path.isdir(next_path):\n",
    "    os.mkdir(next_path)\n",
    "next_path = os.path.join(next_path, tmp_asym)\n",
    "if not os.path.isdir(next_path):\n",
    "    os.mkdir(next_path)\n",
    "file_root = os.path.join(next_path, str(config['trainer']['percent']))\n",
    "if not os.path.isdir(file_root):\n",
    "    os.mkdir(file_root)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Save output_feature, target_noise, label_gt\n",
    "for i in range(num_output):\n",
    "    file_name_data = '%s/%s_feature_4_%s.npy' % (file_root, data_name, str(i))\n",
    "    total_feature = total_final_feature[i].numpy()\n",
    "    np.save(file_name_data , total_feature)\n",
    "\n",
    "file_name_label = '%s/%s_target_noise.npy' % (file_root, data_name)\n",
    "np.save(file_name_label, total_target.detach().cpu())\n",
    "\n",
    "file_name_gt = '%s/%s_label_gt.npy' % (file_root, data_name)\n",
    "np.save(file_name_gt, total_label_gt.detach().cpu())\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "50000\n",
      "50000\n",
      "50000\n"
     ]
    }
   ],
   "source": [
    "#Generate predicted noise index(Unselected)\n",
    "\n",
    "total_index = [i for i in range(len(total_target))]\n",
    "len(total_index)\n",
    "total_index[-1]\n",
    "\n",
    "predicted_noise_layer = []\n",
    "layer_unselected_index = []\n",
    "\n",
    "for layer in layer_selected_index:\n",
    "    tmpp = set(total_index) - set(layer)\n",
    "    layer_unselected_index.append(list(tmpp))\n",
    "\n",
    "print(len(layer_unselected_index[0]) + len(layer_selected_index[0]))\n",
    "print(len(layer_unselected_index[1]) + len(layer_selected_index[1]))\n",
    "print(len(layer_unselected_index[2]) + len(layer_selected_index[2]))\n",
    "\n",
    "for layer in layer_unselected_index:\n",
    "    tmpp_noisy = []\n",
    "    num_noisy = 0\n",
    "    for i in layer:\n",
    "        if total_target[i] != total_label_gt[i]:\n",
    "            num_noisy += 1 \n",
    "            tmpp_noisy.append(1)\n",
    "        else:\n",
    "            tmpp_noisy.append(0)\n",
    "    predicted_noise_layer.append(tmpp_noisy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset: CIFAR10DataLoader, Net: resnet34, NoiseFalse_0.4, Loss: GCELoss\n",
      "================================================== Mahalanobis Distance ==================================================\n",
      "layer4_0 \n",
      "Selected samples by Mahalanobis distance: 34996 \n",
      "Fraction of clean samples/selected samples: 0.6937364270202309\n",
      "10718\n",
      "layer4_1 \n",
      "Selected samples by Mahalanobis distance: 34996 \n",
      "Fraction of clean samples/selected samples: 0.7062807177963195\n",
      "10279\n",
      "layer4_2 \n",
      "Selected samples by Mahalanobis distance: 34996 \n",
      "Fraction of clean samples/selected samples: 0.7173962738598697\n",
      "9890\n",
      "====================================================================================================\n",
      "================================================== Mahalanobis Distance ==================================================\n"
     ]
    }
   ],
   "source": [
    "print('Dataset: {}, Net: {}, Noise{}_{}, Loss: {}'\n",
    "      .format(config['data_loader']['type'], config['arch']['type'], config['trainer']['asym'], config['trainer']['percent'], config['train_loss']['type']))\n",
    "print(\"=\"*50, 'Mahalanobis Distance', \"=\"*50)\n",
    "\n",
    "flag = 0\n",
    "predicted_clean_layer = []\n",
    "\n",
    "for layer in layer_selected_index:\n",
    "    tmp_noisy = []\n",
    "    num_noisy = 0\n",
    "    for i in layer:\n",
    "        if total_target[i] != total_label_gt[i]:\n",
    "            num_noisy +=1 \n",
    "            tmp_noisy.append(1)\n",
    "        else:\n",
    "            tmp_noisy.append(0)\n",
    "    print('layer4_{} \\nSelected samples by Mahalanobis distance: {} \\nFraction of clean samples/selected samples: {}'\n",
    "          .format(flag, len(layer), 1-(num_noisy/len(layer))))\n",
    "    print(num_noisy)\n",
    "    flag += 1\n",
    "    predicted_clean_layer.append(tmp_noisy)\n",
    "# config['trainer']['percent']\n",
    "\n",
    "print(\"=\"*100)\n",
    "print(\"=\"*50, 'Mahalanobis Distance', \"=\"*50)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "recall, specificity, precision, accuracy, frac_clean, sel_samples = [], [], [], [], [], []\n",
    "\n",
    "for i in range(len(predicted_clean_layer)):\n",
    "\n",
    "    tp, fn = sum(predicted_noise_layer[i]), sum(predicted_clean_layer[i])\n",
    "    fp, tn = len(predicted_noise_layer[i]) - tp, len(predicted_clean_layer[i]) - fn\n",
    "\n",
    "    frac_clean.append( round(tn / (fn + tn), 5))\n",
    "    recall.append(round(tp / (tp + fn), 5))\n",
    "    precision.append(round(tp / (tp + fp), 5))\n",
    "    specificity.append(round(tn / (tn + fp), 5))\n",
    "    accuracy.append(round((tp + tn) / (tp + tn + fp + fn), 5))\n",
    "    sel_samples.append(fn + tn)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[34996, 34996, 34996]"
      ]
     },
     "execution_count": 91,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sel_samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.69374, 0.70628, 0.7174]"
      ]
     },
     "execution_count": 92,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "frac_clean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.40555, 0.42989, 0.45147]"
      ]
     },
     "execution_count": 93,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "recall"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.48734, 0.5166, 0.54252]"
      ]
     },
     "execution_count": 94,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "precision"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.7594, 0.77313, 0.7853]"
      ]
     },
     "execution_count": 95,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "specificity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "def report_metric(sel_samples, precision, recall, specificity, accuracy, frac_clean):\n",
    "    print('Dataset: {}, Net: {}, Noise{}_{}, Loss: {}'\n",
    "      .format(config['data_loader']['type'], config['arch']['type'], config['trainer']['asym'], config['trainer']['percent'], config['train_loss']['type']))\n",
    "    print(\"=\"*50, 'Mahalanobis Distance', \"=\"*50)\n",
    "    \n",
    "    if len(recall) > 1:\n",
    "        for i in range(len(recall)):\n",
    "            print('layer4_{} \\nSelected samples by Mahalanobis distance: {} \\nPrecision: {} \\nRecall: {} \\nSpecificity: {}\\nAccuracy: {} \\nFraction of clean samples/selected samples: {}'\n",
    "                  .format(i, sel_samples[i], precision[i], recall[i], specificity[i], accuracy[i], frac_clean[i]))\n",
    "    else:\n",
    "        print('layer4_{} \\nSelected samples by Mahalanobis distance: {} \\nPrecision: {} \\nRecall: {} \\nSpecificity: {}\\nAccuracy: {} \\nFraction of clean samples/selected samples: {}'\n",
    "                  .format(sel_samples, precision, recall, specificity, accuracy, frac_clean))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset: CIFAR10DataLoader, Net: resnet34, NoiseFalse_0.4, Loss: GCELoss\n",
      "================================================== Mahalanobis Distance ==================================================\n",
      "layer4_0 \n",
      "Selected samples by Mahalanobis distance: 34996 \n",
      "Precision: 0.48734 \n",
      "Recall: 0.40555 \n",
      "Specificity: 0.7594\n",
      "Accuracy: 0.6318 \n",
      "Fraction of clean samples/selected samples: 0.69374\n",
      "layer4_1 \n",
      "Selected samples by Mahalanobis distance: 34996 \n",
      "Precision: 0.5166 \n",
      "Recall: 0.42989 \n",
      "Specificity: 0.77313\n",
      "Accuracy: 0.64936 \n",
      "Fraction of clean samples/selected samples: 0.70628\n",
      "layer4_2 \n",
      "Selected samples by Mahalanobis distance: 34996 \n",
      "Precision: 0.54252 \n",
      "Recall: 0.45147 \n",
      "Specificity: 0.7853\n",
      "Accuracy: 0.66492 \n",
      "Fraction of clean samples/selected samples: 0.7174\n"
     ]
    }
   ],
   "source": [
    "report_metric(sel_samples, precision, recall, specificity, accuracy, frac_clean)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "        Metric Layer4_0 Layer4_1 Layer4_2\n",
      "0      Samples    34996    34996    34996\n",
      "1    Precision  0.48734   0.5166  0.54252\n",
      "2       Recall  0.40555  0.42989  0.45147\n",
      "3  Specificity   0.7594  0.77313   0.7853\n",
      "4     Accuracy   0.6318  0.64936  0.66492\n",
      "5     Fraction  0.69374  0.70628   0.7174\n",
      "./saved/mahalanobis/cifar10/resnet34/MultiStepLR/GCELoss/sym/0.4\n"
     ]
    }
   ],
   "source": [
    "# Save as txt\n",
    "df = pd.DataFrame(columns = ['Layer4_0', 'Layer4_1', 'Layer4_2'])\n",
    "\n",
    "df.loc[len(df)] = sel_samples\n",
    "df.loc[len(df)] = precision\n",
    "df.loc[len(df)] = recall\n",
    "df.loc[len(df)] = specificity\n",
    "df.loc[len(df)] = accuracy\n",
    "df.loc[len(df)] = frac_clean\n",
    "df.insert(0, 'Metric', ['Samples', 'Precision', 'Recall', 'Specificity', 'Accuracy', 'Fraction'])\n",
    "\n",
    "print(df)\n",
    "print(file_root)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv(file_root+'/metric.txt', index=False, header=True, sep=\"\\t\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
